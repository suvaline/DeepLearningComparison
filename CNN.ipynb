{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suvaline/DeepLearningComparison/blob/master/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZGn0y-eo2QX",
        "colab_type": "code",
        "outputId": "a7dd892b-ced8-479d-9480-23805cb6b18c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#imports\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from keras.datasets import mnist, cifar10, cifar100\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0BGIstnpVW2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8007df5c-1e1e-4c91-d2f7-47476af90536"
      },
      "source": [
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 10\n",
        "dataset_name = \"mnist\" \n",
        "max_pooling = False\n",
        "dropout = False\n",
        "# input image dimensions\n",
        "if(dataset_name == \"mnist\"):\n",
        "  img_rows, img_cols = 28, 28\n",
        "  (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "  color_count = 1\n",
        "if(dataset_name == \"cifar10\"):\n",
        "  img_rows, img_cols = 32, 32\n",
        "  (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "  color_count = 3\n",
        "if(dataset_name == \"cifar100\"):\n",
        "  num_classes = 100\n",
        "  img_rows, img_cols = 32, 32\n",
        "  (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "  color_count = 3\n",
        "# the data, split between train and test sets\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljm7sFSuwsxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def create_2d_array(x,y):\n",
        "  a = [0] * x\n",
        "  for i in range(x):\n",
        "    a[i] = [0] * y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znzz4uY5pbys",
        "colab_type": "code",
        "outputId": "8becd71b-9c88-44ac-e322-64b183d72aa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], color_count, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], color_count, img_rows, img_cols)\n",
        "    input_shape = (color_count, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, color_count)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, color_count)\n",
        "    input_shape = (img_rows, img_cols, color_count)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "def cnn(depth, width, input_shape, num_classes, max_pooling = False, dropout = False, kernel_size = (3, 3)):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(width, kernel_size, activation='relu', input_shape = input_shape))\n",
        "  \n",
        "  i = 0\n",
        "  while i<depth-1:\n",
        "    model.add(Conv2D(width, kernel_size, activation='relu'))\n",
        "    if max_pooling:\n",
        "      model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    if dropout:\n",
        "      model.add(Dropout(0.4))\n",
        "    i += 1\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(num_classes, activation='softmax'))\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ9ppwieO5iz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_2d_array(x,y):\n",
        "  a = [0] * x\n",
        "  for i in range(x):\n",
        "    a[i] = [0] * y\n",
        "  return a\n",
        "def plot(history):\n",
        "  plt.figure(figsize=[8,6])\n",
        "  plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
        "  plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
        "  plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
        "  plt.xlabel('Epochs ',fontsize=16)\n",
        "  plt.ylabel('Loss',fontsize=16)\n",
        "  plt.title('Loss Curves',fontsize=16)\n",
        "  plt.show()\n",
        " \n",
        "#Plot the Accuracy Curves\n",
        "  plt.figure(figsize=[8,6])\n",
        "  plt.plot(history.history['acc'],'r',linewidth=3.0)\n",
        "  plt.plot(history.history['val_acc'],'b',linewidth=3.0)\n",
        "  plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
        "  plt.xlabel('Epochs ',fontsize=16)\n",
        "  plt.ylabel('Accuracy',fontsize=16)\n",
        "  plt.title('Accuracy Curves',fontsize=16)\n",
        "  plt.show()\n",
        "\n",
        "def create_heatmaps(rows, columns, array, create_max_heatmap = 'false'):\n",
        "  print(\"mean heatmap:\")\n",
        "  mean_array = np.mean(array, axis=2)\n",
        "  ax = sns.heatmap(mean_array, linewidth=0.5,vmin=0, vmax=1, annot=True)\n",
        "  plt.show()\n",
        "  last_loss_array = create_2d_array(rows, columns)\n",
        "  for i in range(rows):\n",
        "    for j in range(columns):\n",
        "      last_loss_array[i][j] = array[i][j][epochs-1]\n",
        "  \n",
        "  print(\"last result heatmap:\")\n",
        "  ax = sns.heatmap(last_loss_array, linewidth=0.5,vmin=0, vmax=1, annot=True)\n",
        "  plt.show()\n",
        "  if create_max_heatmap == 'true':\n",
        "    print(\"max heatmap :\")\n",
        "    max_array = np.amax(array, axis=2)\n",
        "    ax = sns.heatmap(max_array, linewidth=0.5,vmin=0, vmax=1, annot=True)\n",
        "    plt.show()\n",
        "\n",
        "def show_heatmaps(arrays, depth, width):\n",
        "  loss_array = arrays[0]\n",
        "  val_loss_array = arrays[1]\n",
        "  acc_array = arrays[2]\n",
        "  val_acc_array = arrays[3]\n",
        "  print(\"loss heatmaps :\")\n",
        "  create_heatmaps(depth, width, loss_array)\n",
        "  print(\"validation loss heatmaps :\")\n",
        "  create_heatmaps(depth, width, val_loss_array)\n",
        "  print(\"accuracy heatmaps :\")\n",
        "  create_heatmaps(depth, width, acc_array, create_max_heatmap = 'true')\n",
        "  print(\"validation accuracy heatmaps :\")\n",
        "  create_heatmaps(depth, width, val_acc_array, create_max_heatmap = 'true')\n",
        "def generate_train_and_plot(depth, multiplier, arrays, max_depth, width_multiplier):\n",
        "  #depth = 0\n",
        "  #loss_array = create_2d_array(max_depth, width_multiplier)\n",
        "  #val_loss_array = create_2d_array(max_depth, width_multiplier)\n",
        "  #acc_array = create_2d_array(max_depth, width_multiplier)\n",
        "  #val_acc_array = create_2d_array(max_depth, width_multiplier)\n",
        "  original_multiplier = multiplier\n",
        "  loss_array = arrays[0]\n",
        "  val_loss_array = arrays[1]\n",
        "  acc_array = arrays[2]\n",
        "  val_acc_array = arrays[3]\n",
        "  while depth < max_depth:\n",
        "    multiplier = original_multiplier\n",
        "    while multiplier < width_multiplier:\n",
        "      model = cnn(depth*2, 5 + 40 * multiplier, input_shape, num_classes, max_pooling = max_pooling, dropout = dropout)\n",
        "      model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                    optimizer=keras.optimizers.Adadelta(),\n",
        "                    metrics=['accuracy'])\n",
        "      model.summary()\n",
        "      history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "      print(\"depth : \", depth, \"multiplier\", multiplier)\n",
        "      loss_array[depth][multiplier] = history.history['loss']\n",
        "      print(\"loss_array =\", loss_array)\n",
        "      val_loss_array[depth][multiplier] = history.history['val_loss']\n",
        "      print(\"val_loss_array =\", val_loss_array)\n",
        "      acc_array[depth][multiplier] = history.history['acc']\n",
        "      print(\"accuracy_array =\", acc_array)\n",
        "      val_acc_array[depth][multiplier] = history.history['val_acc']\n",
        "      print(\"val_accuracy_array =\", val_acc_array)\n",
        "      multiplier += 1\n",
        "      keras.backend.clear_session()\n",
        "    depth += 1\n",
        "    print(\"depth : \", depth)\n",
        "  arrays = [loss_array, val_loss_array, acc_array, val_acc_array]\n",
        "  return arrays\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9RyHKiSQJKJ",
        "colab_type": "code",
        "outputId": "9f3efe55-730f-4bb7-e651-08e2bbcee06d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "source": [
        "max_depth = 5\n",
        "width_multiplier = 5\n",
        "arrays = [create_2d_array(max_depth, width_multiplier), create_2d_array(max_depth, width_multiplier), create_2d_array(max_depth, width_multiplier), create_2d_array(max_depth, width_multiplier)]\n",
        "arrays = generate_train_and_plot(0,0,arrays,max_depth,width_multiplier)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 5)         50        \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3380)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                33810     \n",
            "=================================================================\n",
            "Total params: 33,860\n",
            "Trainable params: 33,860\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.3962 - acc: 0.8884 - val_loss: 0.2476 - val_acc: 0.9330\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.2236 - acc: 0.9373 - val_loss: 0.2001 - val_acc: 0.9447\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.1758 - acc: 0.9496 - val_loss: 0.1584 - val_acc: 0.9574\n",
            "Epoch 4/10\n",
            "42368/60000 [====================>.........] - ETA: 0s - loss: 0.1471 - acc: 0.9581"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-eabf6afb804b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwidth_multiplier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcreate_2d_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth_multiplier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_2d_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth_multiplier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_2d_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth_multiplier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_2d_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth_multiplier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_train_and_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwidth_multiplier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-a7b13f7c1725>\u001b[0m in \u001b[0;36mgenerate_train_and_plot\u001b[0;34m(depth, multiplier, arrays, max_depth, width_multiplier)\u001b[0m\n\u001b[1;32m     79\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m           validation_data=(x_test, y_test))\n\u001b[0m\u001b[1;32m     82\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"depth : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiplier\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0mloss_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmultiplier\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB1i4JE8Ftgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_heatmaps(arrays, max_depth, width_multiplier)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}