{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gru_and_lstm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suvaline/DeepLearningComparison/blob/master/gru_and_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2zu_y3m4uqw",
        "colab_type": "code",
        "outputId": "40a70b46-2f51-471c-922f-e62d4fd0feef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "!pip install numpy==1.16.1\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, GRU, Embedding, Dropout\n",
        "from tensorflow.python.keras.optimizers import Adam, Adadelta\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "import keras\n",
        "from keras.utils import to_categorical\n",
        "from keras.datasets import imdb, reuters"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.16.1 in /usr/local/lib/python3.6/dist-packages (1.16.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uwsOOtaAqwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 128\n",
        "num_words = 10000\n",
        "dataset = 'reuters'\n",
        "if dataset == 'imdb':\n",
        "  (x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n",
        "                                                      num_words=num_words,\n",
        "                                                      skip_top=0,\n",
        "                                                      maxlen=None,\n",
        "                                                      seed=113,\n",
        "                                                      start_char=1,\n",
        "                                                      oov_char=2,\n",
        "                                                      index_from=3)\n",
        "if dataset == 'reuters':\n",
        "  (x_train, y_train), (x_test, y_test) = reuters.load_data(path=\"reuters.npz\",\n",
        "                                                         num_words=num_words,\n",
        "                                                         skip_top=0,\n",
        "                                                         maxlen=None,\n",
        "                                                         test_split=0.2,\n",
        "                                                         seed=113,\n",
        "                                                         start_char=1,\n",
        "                                                         oov_char=2,\n",
        "                                                         index_from=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I97fxHJvn4qw",
        "colab_type": "code",
        "outputId": "8979799f-1083-4ee5-9994-eb6c9f556567",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"Train-set size: \", len(x_train))\n",
        "print(\"Test-set size:  \", len(x_test))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train-set size:  8982\n",
            "Test-set size:   2246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdBNv2UEp5i_",
        "colab_type": "code",
        "outputId": "c52798db-0e7e-43af-c0d1-0a3b0e01ef8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "if dataset == 'imdb':\n",
        "  num_tokens = [len(tokens) for tokens in x_train + x_test]\n",
        "if dataset == 'reuters':\n",
        "  num_tokens = [len(tokens) for tokens in x_train]\n",
        "  \n",
        "  train_binary = to_categorical(y_train)\n",
        "  val_binary = to_categorical(y_test)\n",
        "num_tokens = np.array(num_tokens)\n",
        "print(np.mean(num_tokens))\n",
        "print(np.max(num_tokens))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "145.5398574927633\n",
            "2376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npRU-5QFqZJC",
        "colab_type": "code",
        "outputId": "b8501656-96ac-4acb-f273-f980c2509163",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
        "max_tokens = int(max_tokens)\n",
        "max_tokens"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "437"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8t_6ZReywH5",
        "colab_type": "text"
      },
      "source": [
        "**Padding and Truncating Data**\n",
        "\n",
        "Prepadding data ensures that the padding has no influence on the networks state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqHD_E9wyEXu",
        "colab_type": "code",
        "outputId": "b81c41f8-f73a-4d5c-ac74-42e11fe6ea5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        }
      },
      "source": [
        "print(x_train[1])\n",
        "print(np.sum(num_tokens < max_tokens) / len(num_tokens))\n",
        "pad = 'post'\n",
        "x_train_pad = pad_sequences(x_train, maxlen=max_tokens, padding=pad, truncating=pad)\n",
        "x_test_pad = pad_sequences(x_test, maxlen=max_tokens, padding=pad, truncating=pad)\n",
        "print(x_train_pad[1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 3267, 699, 3434, 2295, 56, 2, 7511, 9, 56, 3906, 1073, 81, 5, 1198, 57, 366, 737, 132, 20, 4093, 7, 2, 49, 2295, 2, 1037, 3267, 699, 3434, 8, 7, 10, 241, 16, 855, 129, 231, 783, 5, 4, 587, 2295, 2, 2, 775, 7, 48, 34, 191, 44, 35, 1795, 505, 17, 12]\n",
            "0.9441104431084391\n",
            "[   1 3267  699 3434 2295   56    2 7511    9   56 3906 1073   81    5\n",
            " 1198   57  366  737  132   20 4093    7    2   49 2295    2 1037 3267\n",
            "  699 3434    8    7   10  241   16  855  129  231  783    5    4  587\n",
            " 2295    2    2  775    7   48   34  191   44   35 1795  505   17   12\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ9T9-vh06-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "embedding_size = 8\n",
        "def gru(num_words, embedding_size, max_tokens):\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Embedding(input_dim=num_words,\n",
        "                      output_dim=embedding_size,\n",
        "                      input_length=max_tokens,\n",
        "                      name='layer_embedding'))\n",
        "\n",
        "  model.add(tf.keras.layers.CuDNNGRU(units=16, return_sequences=True))\n",
        "\n",
        "  model.add(tf.keras.layers.CuDNNGRU(units=8, return_sequences=True))\n",
        "\n",
        "  model.add(tf.keras.layers.CuDNNGRU(units=4))\n",
        "\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  optimizer = Adam(lr=1e-3)\n",
        "\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer=optimizer,\n",
        "                metrics=['accuracy'])\n",
        "  return model\n",
        "def gru2(num_words, embedding_size, max_tokens, units, depth):\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Embedding(input_dim=num_words,\n",
        "                      output_dim=embedding_size,\n",
        "                      input_length=max_tokens,\n",
        "                      name='layer_embedding'))\n",
        "\n",
        "  i = 0\n",
        "  while i < depth:\n",
        "      model.add(tf.keras.layers.CuDNNGRU(units=units, return_sequences=True))\n",
        "      i += 1\n",
        "\n",
        "  model.add(tf.keras.layers.CuDNNGRU(units=units))\n",
        "\n",
        "\n",
        "  \n",
        "  if dataset == 'imdb':\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    optimizer = Adam(lr=1e-3)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=['accuracy'])\n",
        "  if dataset == 'reuters':\n",
        "    model.add(Dense(46, activation='softmax'))\n",
        "    optimizer = Adadelta()\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=['accuracy'])\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VEtyDBRBcaB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEVwYpVhVrpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0_KWNJVUvlV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "embedding_size = 8\n",
        "def lstm(num_words, embedding_size, max_tokens):\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Embedding(input_dim=num_words,\n",
        "                      output_dim=embedding_size,\n",
        "                      input_length=max_tokens,\n",
        "                      name='layer_embedding'))\n",
        "\n",
        "  model.add(tf.keras.layers.CuDNNLSTM(units=15, return_sequences=True))\n",
        "\n",
        "  model.add(tf.keras.layers.CuDNNLSTM(units=8, return_sequences=True))\n",
        "\n",
        "  model.add(tf.keras.layers.CuDNNLSTM(units=4))\n",
        "\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  optimizer = Adam(lr=1e-3)\n",
        "\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "def lstm2(num_words, embedding_size, max_tokens, units, depth):\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Embedding(input_dim=num_words,\n",
        "                      output_dim=embedding_size,\n",
        "                      input_length=max_tokens,\n",
        "                      name='layer_embedding'))\n",
        "  i = 0\n",
        "  while i < depth:\n",
        "      model.add(tf.keras.layers.CuDNNLSTM(units=units, return_sequences=True))\n",
        "      i += 1\n",
        "\n",
        "  model.add(tf.keras.layers.CuDNNLSTM(units=units))\n",
        "\n",
        "  if dataset == 'imdb':\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    optimizer = Adam(lr=1e-3)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=['accuracy'])\n",
        "  if dataset == 'reuters':\n",
        "    model.add(Dense(46, activation='softmax'))\n",
        "    optimizer = Adadelta()\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=['accuracy'])\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srd42Vn3T5DV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKj3nW35AoAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4G411qflSTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_2d_array(x,y):\n",
        "  a = [0] * x\n",
        "  for i in range(x):\n",
        "    a[i] = [0] * y\n",
        "  return a\n",
        "def plot(history):\n",
        "  plt.figure(figsize=[8,6])\n",
        "  plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
        "  plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
        "  plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
        "  plt.xlabel('Epochs ',fontsize=16)\n",
        "  plt.ylabel('Loss',fontsize=16)\n",
        "  plt.title('Loss Curves',fontsize=16)\n",
        "  plt.show()\n",
        "def plot(history):\n",
        "  plt.figure(figsize=[8,6])\n",
        "  plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
        "  plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
        "  plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
        "  plt.xlabel('Epochs ',fontsize=16)\n",
        "  plt.ylabel('Loss',fontsize=16)\n",
        "  plt.title('Loss Curves',fontsize=16)\n",
        "  plt.show()\n",
        " \n",
        "#Plot the Accuracy Curves\n",
        "  plt.figure(figsize=[8,6])\n",
        "  plt.plot(history.history['acc'],'r',linewidth=3.0)\n",
        "  plt.plot(history.history['val_acc'],'b',linewidth=3.0)\n",
        "  plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
        "  plt.xlabel('Epochs ',fontsize=16)\n",
        "  plt.ylabel('Accuracy',fontsize=16)\n",
        "  plt.title('Accuracy Curves',fontsize=16)\n",
        "  plt.show()\n",
        "\n",
        "def create_heatmaps(rows, columns, array, create_max_heatmap = 'false'):\n",
        "  print(\"mean loss heatmap:\")\n",
        "  mean_array = np.mean(array, axis=2)\n",
        "  ax = sns.heatmap(mean_array, linewidth=0.5,vmin=0, vmax=1, annot=True)\n",
        "  plt.show()\n",
        "  last_loss_array = create_2d_array(rows, columns)\n",
        "  for i in range(rows):\n",
        "    for j in range(columns):\n",
        "      last_loss_array[i][j] = array[i][j][epochs-1]\n",
        "  \n",
        "  print(\"last result heatmap:\")\n",
        "  ax = sns.heatmap(last_loss_array, linewidth=0.5,vmin=0, vmax=1, annot=True)\n",
        "  plt.show()\n",
        "  if create_max_heatmap == 'true':\n",
        "    print(\"max heatmap :\")\n",
        "    max_array = np.amax(array, axis=2)\n",
        "    ax = sns.heatmap(max_array, linewidth=0.5,vmin=0, vmax=1, annot=True)\n",
        "    plt.show()\n",
        "\n",
        "def show_heatmaps(arrays, depth, width):\n",
        "  loss_array = arrays[0]\n",
        "  val_loss_array = arrays[1]\n",
        "  acc_array = arrays[2]\n",
        "  val_acc_array = arrays[3]\n",
        "  print(\"loss heatmaps :\")\n",
        "  create_heatmaps(depth, width, loss_array)\n",
        "  print(\"validation loss heatmaps :\")\n",
        "  create_heatmaps(depth, width, val_loss_array)\n",
        "  print(\"accuracy heatmaps :\")\n",
        "  create_heatmaps(depth, width, acc_array, create_max_heatmap = 'true')\n",
        "  print(\"validation accuracy heatmaps :\")\n",
        "  create_heatmaps(depth, width, val_acc_array, create_max_heatmap = 'true')\n",
        "def generate_train_and_plot(depth, multiplier, arrays, max_depth, width_multiplier, network_type):\n",
        "  original_multiplier = multiplier\n",
        "  loss_array = arrays[0]\n",
        "  val_loss_array = arrays[1]\n",
        "  acc_array = arrays[2]\n",
        "  val_acc_array = arrays[3]\n",
        "  while depth < max_depth:\n",
        "    multiplier = original_multiplier\n",
        "    while multiplier < width_multiplier:\n",
        "      if network_type == 'GRU':\n",
        "        model = gru2(num_words, embedding_size, max_tokens,  5 + 20 * multiplier, depth)\n",
        "      if network_type == 'LSTM':\n",
        "        model = lstm2(num_words, embedding_size, max_tokens,  5 + 20 * multiplier, depth)\n",
        "      model.summary()\n",
        "      if dataset == 'imdb':\n",
        "        history = model.fit(x_train_pad, y_train, validation_split=0.05, epochs=epochs, batch_size=batch_size)\n",
        "      if dataset == 'reuters':\n",
        "        history = model.fit(x_train_pad, train_binary, validation_data=(x_test_pad, val_binary), epochs=epochs, batch_size=batch_size)\n",
        "      tf.keras.backend.clear_session()\n",
        "      print(\"depth :\", depth, \"multiplier\", multiplier)\n",
        "      loss_array[depth][multiplier] = history.history['loss']\n",
        "      print(\"loss_array =\", loss_array)\n",
        "      val_loss_array[depth][multiplier] = history.history['val_loss']\n",
        "      print(\"val_loss_array =\", val_loss_array)\n",
        "      acc_array[depth][multiplier] = history.history['acc']\n",
        "      print(\"accuracy_array =\", acc_array)\n",
        "      val_acc_array[depth][multiplier] = history.history['val_acc']\n",
        "      print(\"val_accuracy_array =\", val_acc_array)\n",
        "      multiplier += 1\n",
        "    depth += 1\n",
        "    print(\"depth : \", depth)\n",
        "  arrays = [loss_array, val_loss_array, acc_array, val_acc_array]\n",
        "  return arrays\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMtUVHj_SGGo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "outputId": "f8017f1f-57ce-46b0-bf55-119d7ad03f74"
      },
      "source": [
        "#max_depth describes the layer count , width_multiplier describes the amount of steps for the network width \n",
        "max_depth = 3\n",
        "width_multiplier = 5\n",
        "epochs = 10\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "arrays = [create_2d_array(max_depth, width_multiplier), create_2d_array(max_depth, width_multiplier), create_2d_array(max_depth, width_multiplier), create_2d_array(max_depth, width_multiplier)]\n",
        "arrays = generate_train_and_plot(0, 0, arrays, max_depth, width_multiplier, 'LSTM')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer_embedding (Embedding)  (None, 437, 8)            80000     \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm (CuDNNLSTM)       (None, 5)                 300       \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 46)                276       \n",
            "=================================================================\n",
            "Total params: 80,576\n",
            "Trainable params: 80,576\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 8982 samples, validate on 2246 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/10\n",
            "8982/8982 [==============================] - 5s 564us/sample - loss: 3.2441 - acc: 0.2110 - val_loss: 2.4785 - val_acc: 0.3620\n",
            "Epoch 2/10\n",
            "8982/8982 [==============================] - 1s 164us/sample - loss: 2.4350 - acc: 0.3517 - val_loss: 2.4234 - val_acc: 0.3620\n",
            "Epoch 3/10\n",
            "8982/8982 [==============================] - 1s 164us/sample - loss: 2.4096 - acc: 0.3517 - val_loss: 2.4165 - val_acc: 0.3620\n",
            "Epoch 4/10\n",
            "8982/8982 [==============================] - 1s 163us/sample - loss: 2.4056 - acc: 0.3517 - val_loss: 2.4160 - val_acc: 0.3620\n",
            "Epoch 5/10\n",
            "8982/8982 [==============================] - 1s 164us/sample - loss: 2.4039 - acc: 0.3517 - val_loss: 2.4168 - val_acc: 0.3620\n",
            "Epoch 6/10\n",
            "3584/8982 [==========>...................] - ETA: 0s - loss: 2.3555 - acc: 0.3644"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e015db61a8e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcreate_2d_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth_multiplier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_2d_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth_multiplier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_2d_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth_multiplier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_2d_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth_multiplier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_train_and_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth_multiplier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-b6ac86d7aa41>\u001b[0m in \u001b[0;36mgenerate_train_and_plot\u001b[0;34m(depth, multiplier, arrays, max_depth, width_multiplier, network_type)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'reuters'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_binary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_binary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"depth :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiplier\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}